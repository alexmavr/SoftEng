\documentclass[a4paper,10pt]{article} \usepackage{anysize}
\marginsize{2cm}{2cm}{1cm}{1cm}
%\textwidth 6.0in \textheight = 664pt
\usepackage{xltxtra}
\usepackage{xunicode}
\usepackage{graphicx}
\usepackage{color}
\usepackage[table]{xcolor}
%\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage{xgreek}
\usepackage{fancyvrb}
\usepackage{minted}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{relsize}
\usepackage{float} 
\setmainfont[Mapping=TeX-text]{CMU Serif}
\begin{document}

\definecolor{bootred}{RGB}{248,148,6}
\definecolor{bootgreen}{RGB}{81,163,81}
\definecolor{bootblue}{RGB}{73, 178, 205}

\include{title/title}

\section{Overview}
    Instead of the default Software Engineering Course project we present
    this following project proposal. With the advances in computer science and
    the development of the World Wide Web it has been clear that the amount
    of information one can find is more than one can process. What's more, it's
    hard to monitor websites for topics of interest manually, especially due to
    the fact that their content may have a much more broad spectrum than one's
    interests. In addition, the number of sites that might contain useful
    information is vast. As a result we rely on search engines to provide us
    with a limited number of web pages based on our own search criteria. This
    approach serves us well when one actually wants to find some specific
    pieces of information. However it is seriously lacking when one simply
    wants to get new pieces of information on his subjects of interest as they
    appear. 

    This project aims to address the aforementioned issue in a different
    manner. Instead of searching on one site, we can monitor sites of
    interest and filter their updates based on keywords provided. If the
    updates match the search criteria then the end user is notified for the
    latest developments. The proposed project is a meta-search engine used to
    monitor sites for required information and provide notifications to its
    users accordingly. 

\section{Components}
    The project consists of the following three main components.
    \begin{itemize}
        \item A web application framework providing the user interface and the
            database handling.
        \item A distributed data collection system (crawler network).
        \item A notification system.
    \end{itemize}
    \subsection{Framework}
        The framework allows the developer to produce a configuration of the
        web application in order to provide an interface to the end user,
        declare the desired sites to be monitored and define the methods of
        the searching process.
    \subsection{Data Collection System}
        The data collection system is a distributed network of crawlers which
        poll the server on predefined intervals in order to receive a
        workload. They then proceed to process the workload by searching on
        the sites for the required data and send the results back to the
        server.
    \subsection{Notification System}
        The notification system provides email services. When a notification
        is triggered, the notification system is to send an email to the end
        user with the information he requested.
\section{Interactions}
    \subsection{Generic Use Cases}
        \subsubsection{Developer Case}
            The developer configures the framework to provide a front-end to
            the end user and a collection of websites which will be crawled
            for the requested data. For each website, he will need to provide
            the corresponding instructions so the crawlers will be able to
            locate the data of interest. Additionally, he has full control
            over which subsets of the website collection will be available to
            the end user.
        \subsubsection{End User Case}
            The end user interacts with the web application and the
            notification system. He specifies the search terms on the web user
            interface and selects the sites to be monitored, as configured
            appropriately by the developer. The framework then handles the
            user request and forwards the necessary information to the crawler
            network. When an appropriate result is found, the user is
            contacted by the notification system. 
    \subsection{End User Scenarios}
        None yet. Feel free to add some.

\section{High-Level Description}
    None yet. Feel free to add some.

\section{Specifications}
    This is where we should describe deliverables
    \begin{itemize}
            \item The generic web framework in the form of a django application
            \item The crawler implementation.
            \item The notification system.
            \item The above configured as a usage example along with the user
                front end running on a production server.
    \end{itemize}
    None yet. Feel free to add some.


\end{document}
